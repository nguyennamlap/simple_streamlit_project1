import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set style for better-looking plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

def load_data(file_path='data/application_train.csv'):
    """Load the processed data"""
    df = pd.read_csv(file_path)
    print(f"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")
    return df

def basic_info(df):
    """Display basic information about the dataset"""
    print("\n" + "="*80)
    print("BASIC INFORMATION")
    print("="*80)
    
    print("\nS·ªë c·ªôt v√† h√†ng", df.shape)
    print("\nKi·ªÉu d·ªØ li·ªáu c·ªßa c√°c c·ªôt:")
    print(df.dtypes.value_counts())
    
    print("\nTH√¥ng tin t·ªïng quan v·ªÅ d·ªØ li·ªáu 5 d√≤ng ƒë·∫ßu:")
    print(df.head())

def missing_values_analysis(df,path):
    """Analyze missing values in the dataset"""
    print("\n" + "="*80)
    print("MISSING VALUES ANALYSIS")
    print("="*80)
    
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100 # t·ªâ l·ªá thi·∫øu d·ªØ li·ªáu theo ph·∫ßn trƒÉm
    missing_df = pd.DataFrame({
        'Column': missing.index,
        'Missing_Count': missing.values,
        'Missing_Percentage': missing_pct.values
    })
    # s·∫Øp x·∫øp theo t·ªâ l·ªá thi·∫øu d·ªØ li·ªáu gi·∫£m d·∫ßn v√† l·ªçc ch·ªâ nh·ªØng c·ªôt c√≥ thi·∫øu d·ªØ li·ªáu
    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values(
        'Missing_Percentage', ascending=False
    )
    # Hi·ªÉn th·ªã b·∫£ng thi·∫øu d·ªØ li·ªáu
    if len(missing_df) > 0:
        print("\nColumns with missing values:")
        # chuy·ªÉn ƒë·ªïi hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß b·∫£ng kh√¥ng b·ªã c·∫Øt b·ªõt khi in ra
        print(missing_df.to_string(index=False))
        
        # Plot missing values
        plt.figure(figsize=(12, 6))
        top_missing = missing_df.head(20)
        plt.barh(top_missing['Column'], top_missing['Missing_Percentage'], color='salmon', edgecolor='black', alpha=0.7)
        plt.xlabel('Missing Percentage (%)')
        plt.title('Top 20 c·ªôt d·ªØ li·ªáu b·ªã thi·∫øu nhi·ªÅu nh·∫•t')
        plt.tight_layout()
        plt.savefig(f'{path}/missing_values.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("\nƒê√£ l∆∞u bi·ªÉu ƒë·ªì thi·∫øu d·ªØ li·ªáu v√†o 'images/before_train/missing_values.png'")
    else:
        print("\nNo missing values found!")

def target_analysis_plot(df, target_col, path):
    print("\n" + "="*80)
    print(f"C·ªôt ƒë∆∞·ª£c ph√¢n t√≠ch l√†: ({target_col})")
    print("="*80)

    if target_col not in df.columns:
        print(f"\nTarget column '{target_col}' not found in dataset")
        return

    # Th·ªëng k√™
    print("\nC√°c gi√° tr·ªã c·ªßa c·ªôt m·ª•c ti√™u:")
    print(df[target_col].value_counts())

    print("\nT·ª∑ l·ªá c√°c gi√° tr·ªã trong c·ªôt m·ª•c ti√™u:")
    print(df[target_col].value_counts(normalize=True))

    # Chu·∫©n b·ªã d·ªØ li·ªáu
    counts = df[target_col].value_counts()
    colors = ['#27ae60', '#c0392b']

    # T·∫°o figure & axis (1 axis duy nh·∫•t)
    fig, ax = plt.subplots(figsize=(7, 5))

    # V·∫Ω bi·ªÉu ƒë·ªì
    counts.plot(
        kind='bar',
        ax=ax,
        color=colors,
        edgecolor='black',
        alpha=0.9,
        width=0.7
    )

    # Trang tr√≠ bi·ªÉu ƒë·ªì
    ax.set_title(
        f'Ph√¢n B·ªë Bi·∫øn M·ª•c Ti√™u c·ªßa c·ªôt {target_col}',
        fontsize=13,
        fontweight='bold',
        pad=12
    )
    ax.set_xlabel(f'C√°c L·ªõp M·ª•c Ti√™u c·ªßa c·ªôt {target_col}', fontsize=11)
    ax.set_ylabel('Count', fontsize=11)
    ax.set_xticklabels(['Kh√¥ng V·ª° N·ª£ (0)', 'V·ª° N·ª£ (1)'], rotation=0)

    # Ghi s·ªë l∆∞·ª£ng tr√™n c·ªôt
    for i, v in enumerate(counts): # i: v·ªã tr√≠ c·ªôt, v: gi√° tr·ªã c·ªôt
        ax.text(
            i,
            v + max(counts) * 0.01,
            f'{v:,}', # ƒë·ªãnh d·∫°ng s·ªë c√≥ d·∫•u ph·∫©y
            ha='center',
            fontweight='bold' # cƒÉn gi·ªØa v√† in ƒë·∫≠m
        )

    ax.grid(axis='y', linestyle='--', alpha=0.7)
    # l∆∞·ªõi ngang ch·ªâ theo tr·ª•c y
    # L∆∞u & ƒë√≥ng
    plt.tight_layout()
    plt.savefig(f"{path}/{target_col}_bar_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()

def target_analysis_pie(df, target_col, path):
    print("\n" + "="*80)
    print(f"C·ªôt ƒë∆∞·ª£c ph√¢n t√≠ch l√†: ({target_col})")
    print("="*80)

    if target_col not in df.columns:
        print(f"\nTarget column '{target_col}' not found in dataset")
        return

    # Th·ªëng k√™
    print("\nC√°c gi√° tr·ªã c·ªßa c·ªôt m·ª•c ti√™u:")
    print(df[target_col].value_counts())

    print("\nT·ª∑ l·ªá c√°c gi√° tr·ªã trong c·ªôt m·ª•c ti√™u:")
    print(df[target_col].value_counts(normalize=True))
    
    # L·∫•y s·ªë l∆∞·ª£ng gi√° tr·ªã duy nh·∫•t
    n_unique = df[target_col].nunique()
    
    # T·∫°o m√†u s·∫Øc d·ª±a tr√™n s·ªë l∆∞·ª£ng gi√° tr·ªã
    if n_unique == 2:
        colors = ['#27ae60', '#c0392b']  # Xanh l√°, ƒê·ªè
    elif n_unique == 3:
        colors = ['#27ae60', '#f39c12', '#c0392b']  # Xanh l√°, Cam, ƒê·ªè
    elif n_unique == 4:
        colors = ['#27ae60', '#3498db', '#f39c12', '#c0392b']  # Xanh l√°, Xanh d∆∞∆°ng, Cam, ƒê·ªè
    elif n_unique == 5:
        colors = ['#27ae60', '#3498db', '#f1c40f', '#f39c12', '#c0392b']  # Xanh l√°, Xanh d∆∞∆°ng, V√†ng, Cam, ƒê·ªè
    else:
        # S·ª≠ d·ª•ng colormap cho nhi·ªÅu gi√° tr·ªã h∆°n
        cmap = plt.cm.Set3  # Ho·∫∑c plt.cm.tab20, plt.cm.Set2
        colors = [cmap(i) for i in np.linspace(0, 1, n_unique)]
    
    # T√≠nh to√°n k√≠ch th∆∞·ªõc figure d·ª±a tr√™n s·ªë l∆∞·ª£ng gi√° tr·ªã
    if n_unique <= 5:
        figsize = (8, 6)
    elif n_unique <= 8:
        figsize = (10, 7)
    else:
        figsize = (12, 8)
    
    # T·∫°o figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # L·∫•y gi√° tr·ªã v√† nh√£n
    value_counts = df[target_col].value_counts()
    labels = [f"{idx} ({val})" for idx, val in value_counts.items()]
    
    # Pie chart v·ªõi nhi·ªÅu t√πy ch·ªçn
    wedges, texts, autotexts = ax.pie(value_counts, 
                                       labels=labels if n_unique <= 8 else None,
                                       autopct='%1.1f%%', 
                                       colors=colors, 
                                       startangle=90, 
                                       counterclock=False,
                                       wedgeprops={'edgecolor': 'black', 'linewidth': 1, 'alpha': 0.9},
                                       textprops={'fontsize': 10})
    
    # ƒê·ªãnh d·∫°ng autopct (ph·∫ßn trƒÉm)
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(9)
    
    # Ti√™u ƒë·ªÅ
    ax.set_title(f'Ph√¢n Ph·ªëi C·ªôt {target_col}', fontsize=14, fontweight='bold', pad=20)
    
    # Th√™m ch√∫ th√≠ch n·∫øu c√≥ nhi·ªÅu gi√° tr·ªã
    if n_unique > 8:
        # T·∫°o legend v·ªõi layout th√≠ch h·ª£p
        n_cols = 2 if n_unique > 12 else 1
        ax.legend(wedges, labels, 
                  title=f"Gi√° tr·ªã ({n_unique} lo·∫°i)",
                  loc="center left", 
                  bbox_to_anchor=(1, 0, 0.5, 1),
                  fontsize=9,
                  ncol=n_cols)
    
    # Th√™m t·ªïng s·ªë l∆∞·ª£ng
    total = value_counts.sum()
    ax.text(0, -1.2, f'T·ªïng s·ªë: {total:,}', 
            ha='center', fontsize=11, fontweight='bold',
            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(f"{path}/{target_col}_pie_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"\nBi·ªÉu ƒë·ªì ph√¢n ph·ªëi c·ªôt '{target_col}' ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {path}")

def numerical_features_analysis(df: pd.DataFrame, target_col: str, path, key_features: list):
   
    print("\n" + "="*80)
    print("NUMERICAL FEATURES ANALYSIS")
    print("="*80)
    
    # ch·ªçn c·ªôt c√≥ d·∫°ng ch·ªØ, l·∫•y t√™n c·ªôt, b·ªè v√†o list python 
    # categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    # ch·ªçn c·ªôt c√≥ d·∫°ng s·ªë, l·∫•y t√™n c·ªôt, b·ªè v√†o list python 
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # lo·∫°i b·ªè c·ªôt m·ª•c ti√™u kh·ªèi danh s√°ch ph√¢n t√≠ch n·∫øu c√≥

    if target_col in numerical_cols:
        numerical_cols.remove(target_col)
    
    print(f"\nNumber of numerical features: {len(numerical_cols)}")
    print("\nNumerical Features Statistics:")
    print(df[numerical_cols].describe())
    
    # Analyze key features
    # key_features = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 
    #                 'T·ªâ l·ªá vay so v·ªõi nhu c·∫ßu', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']
    available_features = [f for f in key_features if f in df.columns]
    
    if available_features:
        print(f"\nKey Features Analysis:")
        for feature in available_features:
            print(f"\n{feature}:")
            print(f"  Mean: {df[feature].mean():.2f}")
            print(f"  Median: {df[feature].median():.2f}")
            print(f"  Std: {df[feature].std():.2f}")
            print(f"  Min: {df[feature].min():.2f}")
            print(f"  Max: {df[feature].max():.2f}")
        
        # Plot distributions
        n_features = len(available_features)
        n_cols = 3
        n_rows = (n_features + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, 
                                figsize=(15, 4.5 * n_rows),
                                constrained_layout=True)

        # Flatten axes
        axes = axes.flatten() if n_features > 1 else [axes]

        # T·∫°o m√†u gradient ƒë·∫πp
        cmap = plt.cm.coolwarm
        colors = [cmap(i) for i in np.linspace(0.2, 0.8, n_features)]

        for idx, feature in enumerate(available_features):
            if idx < len(axes):
                ax = axes[idx]
                data = df[feature].dropna()
                
                if len(data) == 0:
                    ax.text(0.5, 0.5, 'Kh√¥ng c√≥ d·ªØ li·ªáu', 
                        ha='center', va='center', fontsize=11, color='gray')
                    ax.set_title(f'{feature}\n(No Data)', fontsize=10, color='gray')
                    continue
                
                # T√≠nh to√°n bins t·ªëi ∆∞u
                q75, q25 = np.percentile(data, [75, 25])
                iqr = q75 - q25
                bin_width = 2 * iqr / (len(data) ** (1/3))
                n_bins = int((data.max() - data.min()) / bin_width) if bin_width > 0 else 50
                
                # V·∫Ω histogram
                color = colors[idx % len(colors)]
                n, bins, patches = ax.hist(data, 
                                        bins=min(n_bins, 50),
                                        color=color,
                                        edgecolor='white',
                                        linewidth=1.5,
                                        alpha=0.85,
                                        density=False)
                
                # Th√™m th√¥ng tin th·ªëng k√™
                mean_val = data.mean()
                median_val = data.median()
                
                ax.axvline(mean_val, color='#e74c3c', linestyle='--', 
                        linewidth=2, alpha=0.8, label=f'Mean: {mean_val:.2f}')
                ax.axvline(median_val, color='#2ecc71', linestyle='-', 
                        linewidth=2, alpha=0.6, label=f'Median: {median_val:.2f}')
                
                # ƒê·ªãnh d·∫°ng ƒë·∫πp
                ax.set_title(f'Ph√¢n ph·ªëi: {feature}', 
                            fontsize=12, fontweight='bold', pad=10)
                ax.set_xlabel('Gi√° tr·ªã', fontsize=10)
                ax.set_ylabel('T·∫ßn su·∫•t', fontsize=10)
                ax.grid(axis='y', alpha=0.2, linestyle='--')
                
                # Th√™m text box th·ªëng k√™
                stats_text = f'N={len(data):,}\nMean={mean_val:.2f}\nStd={data.std():.2f}'
                ax.text(0.97, 0.97, stats_text, 
                    transform=ax.transAxes,
                    fontsize=9, verticalalignment='top',
                    horizontalalignment='right',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                # Ch·ªâ th√™m legend n·∫øu c·∫ßn
                if idx < 3:  # Ch·ªâ 3 bi·ªÉu ƒë·ªì ƒë·∫ßu
                    ax.legend(loc='upper right', fontsize=8)

        # ·∫®n c√°c subplot kh√¥ng s·ª≠ d·ª•ng
        for idx in range(len(available_features), len(axes)):
            axes[idx].axis('off')
        
        plt.tight_layout()
        plt.savefig(f'{path}/numerical_distributions.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("\nNumerical distributions plot saved as 'numerical_distributions.png'")

def categorical_features_analysis(df, target_col, path):
    
    # Header v·ªõi format ƒë·∫πp
    print("\n" + "‚ïê" * 80)
    print("üìä PH√ÇN T√çCH ƒê·∫∂C TR∆ØNG PH√ÇN LO·∫†I")
    print("‚ïê" * 80)
    
    # X√°c ƒë·ªãnh c√°c c·ªôt ph√¢n lo·∫°i
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # N·∫øu c√≥ target_col v√† l√† categorical, lo·∫°i b·ªè n√≥
    if target_col in categorical_cols:
        categorical_cols.remove(target_col)
    
    print(f"\nüîç S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i: {len(categorical_cols)}")

    if len(categorical_cols) == 0:
        print("‚ùå Kh√¥ng t√¨m th·∫•y ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i n√†o")
        return
    
    # Danh s√°ch c√°c feature quan tr·ªçng c·∫ßn ph√¢n t√≠ch
    key_cat_features = ['S·ªü h·ªØu xe', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE_ENHANCED', 
                        'OCCUPATION_MISSING_TYPE', 'NAME_INCOME_TYPE', 'NAME_CONTRACT_TYPE']
    
    # Ch·ªâ l·∫•y nh·ªØng feature c√≥ trong dataframe
    available_cat = [f for f in key_cat_features if f in df.columns]
    
    # N·∫øu kh√¥ng c√≥ feature trong danh s√°ch key, l·∫•y t·∫•t c·∫£ categorical features
    if not available_cat:
        available_cat = categorical_cols[:6]  # L·∫•y t·ªëi ƒëa 6 features ƒë·∫ßu ti√™n
    
    print(f"\n‚ú® Ph√¢n t√≠ch {len(available_cat)} ƒë·∫∑c tr∆∞ng quan tr·ªçng:")
    for i, feature in enumerate(available_cat, 1):
        unique_count = df[feature].nunique()
        missing_count = df[feature].isnull().sum()
        missing_pct = (missing_count / len(df)) * 100
        print(f"   {i:2d}. {feature:30s} | Gi√° tr·ªã duy nh·∫•t: {unique_count:3d} | "
              f"Thi·∫øu: {missing_count:5,d} ({missing_pct:5.1f}%)")
    
    # PH·∫¶N 1: TH·ªêNG K√ä CHI TI·∫æT
    print("\n" + "‚îÄ" * 80)
    print("üìà TH·ªêNG K√ä CHI TI·∫æT T·ª™NG ƒê·∫∂C TR∆ØNG")
    print("‚îÄ" * 80)
    
    for feature in available_cat[:5]:  # Hi·ªÉn th·ªã chi ti·∫øt 5 features ƒë·∫ßu
        print(f"\nüìã {feature}:")
        print("-" * 40)
        
        value_counts = df[feature].value_counts(dropna=False)
        value_counts_pct = df[feature].value_counts(normalize=True, dropna=False) * 100
        
        # Hi·ªÉn th·ªã top 10 gi√° tr·ªã
        for i, (value, count) in enumerate(value_counts.head(10).items(), 1):
            pct = value_counts_pct.get(value, 0)
            if pd.isna(value):
                value_str = "NULL/MISSING"
            else:
                value_str = str(value)
            print(f"   {i:2d}. {value_str:30s}: {count:7,d} ({pct:5.1f}%)")
        
        # Th√¥ng tin t·ªïng quan
        print(f"   T·ªïng s·ªë gi√° tr·ªã duy nh·∫•t: {value_counts.shape[0]}")
        if value_counts.shape[0] > 10:
            print(f"   ... v√† {value_counts.shape[0] - 10} gi√° tr·ªã kh√°c")
    
    # PH·∫¶N 2: V·∫º BI·ªÇU ƒê·ªí
    print("\n" + "‚îÄ" * 80)
    print("üé® V·∫º BI·ªÇU ƒê·ªí PH√ÇN PH·ªêI")
    print("‚îÄ" * 80)
    
    n_features = len(available_cat)
    n_cols = 2
    n_rows = min(3, (n_features + n_cols - 1) // n_cols)
    
    # T·∫°o figure v·ªõi layout ƒë·∫πp
    fig = plt.figure(figsize=(18, 6 * n_rows))
    gs = fig.add_gridspec(n_rows, 2, hspace=0.3, wspace=0.2)
    
    for idx, feature in enumerate(available_cat[:n_rows * 2]):  # T·ªëi ƒëa 6 bi·ªÉu ƒë·ªì
        if idx >= n_rows * 2:
            break
            
        row = idx // 2
        col = idx % 2
        
        ax1 = fig.add_subplot(gs[row, col])
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        value_counts = df[feature].value_counts().head(10)
        values = value_counts.index.tolist()
        counts = value_counts.values.tolist()
        
        # T·∫°o m√†u gradient
        if len(counts) > 0:
            colors = plt.cm.Set3(np.linspace(0.2, 0.8, len(counts)))
        else:
            colors = ['#3498db']
        
        # V·∫Ω horizontal bar chart
        bars = ax1.barh(range(len(counts)), counts, color=colors, edgecolor='white', height=0.7)
        
        # ƒê·∫£o ng∆∞·ª£c tr·ª•c y ƒë·ªÉ gi√° tr·ªã l·ªõn nh·∫•t ·ªü tr√™n
        ax1.set_yticks(range(len(counts)))
        ax1.set_yticklabels([str(v)[:30] + ('...' if len(str(v)) > 30 else '') for v in values])
        
        # Th√™m s·ªë li·ªáu tr√™n m·ªói c·ªôt
        total = len(df)
        for i, (bar, count) in enumerate(zip(bars, counts)):
            percentage = (count / total) * 100
            ax1.text(bar.get_width() + max(counts) * 0.01, bar.get_y() + bar.get_height()/2,
                    f'{count:,} ({percentage:.1f}%)', 
                    va='center', fontsize=9, fontweight='bold')
        
        # ƒê·ªãnh d·∫°ng bi·ªÉu ƒë·ªì
        ax1.set_xlabel('S·ªë l∆∞·ª£ng', fontsize=10)
        ax1.set_title(f'üìä {feature}\nTop {len(counts)} gi√° tr·ªã ph·ªï bi·∫øn', 
                     fontsize=12, fontweight='bold', pad=12)
        
        # Th√™m grid nh·∫π
        ax1.grid(axis='x', alpha=0.2, linestyle='--')
        
        # X√≥a khung kh√¥ng c·∫ßn thi·∫øt
        ax1.spines['top'].set_visible(False)
        ax1.spines['right'].set_visible(False)
        
        # Th√™m th√¥ng tin t·ªïng quan
        unique_count = df[feature].nunique()
        missing_count = df[feature].isnull().sum()
        ax1.text(0.02, 0.98, 
                f'Gi√° tr·ªã duy nh·∫•t: {unique_count}\nThi·∫øu: {missing_count:,}',
                transform=ax1.transAxes,
                fontsize=9,
                verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))
    
    # Ti√™u ƒë·ªÅ t·ªïng
    fig.suptitle('PH√ÇN T√çCH PH√ÇN PH·ªêI ƒê·∫∂C TR∆ØNG PH√ÇN LO·∫†I', 
                fontsize=16, fontweight='bold', y=1.02)
    
    plt.tight_layout()
    
    # L∆∞u bi·ªÉu ƒë·ªì n·∫øu c√≥ path
    if path:
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
        plt.savefig(f'{path}/top_10.png', dpi=300, bbox_inches='tight', facecolor='white')
        print(f"\n‚úÖ Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {path}")
    
    
def categorical_target_relationship(df: pd.DataFrame, target_col: str):
    print("\n" + "‚îÄ" * 80)
    print(f"üéØ PH√ÇN T√çCH M·ªêI QUAN H·ªÜ V·ªöI TARGET: {target_col}")
    print("‚îÄ" * 80)
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    available_cat = [col for col in categorical_cols if col != target_col]
    # Ch·ªçn 3 features quan tr·ªçng nh·∫•t ƒë·ªÉ ph√¢n t√≠ch v·ªõi target
    top_features = available_cat[:3]
    
    if top_features:
        fig_target, axes_target = plt.subplots(1, min(3, len(top_features)), 
                                                figsize=(5 * min(3, len(top_features)), 6))
        
        if len(top_features) == 1:
            axes_target = [axes_target]
        
        for idx, feature in enumerate(top_features):
            ax = axes_target[idx] if len(top_features) > 1 else axes_target
            
            # T·∫°o crosstab v·ªõi target
            crosstab = pd.crosstab(df[feature].fillna('MISSING'), 
                                    df[target_col], 
                                    normalize='index') * 100
            
            # V·∫Ω stacked bar chart
            crosstab.plot(kind='bar', ax=ax, stacked=True, 
                            color=['#2ecc71', '#e74c3c'], 
                            edgecolor='black', alpha=0.85)
            
            # ƒê·ªãnh d·∫°ng
            ax.set_title(f'{feature} vs {target_col}', fontsize=12, fontweight='bold', pad=10)
            ax.set_xlabel(feature, fontsize=10)
            ax.set_ylabel('T·ª∑ l·ªá (%)', fontsize=10)
            ax.tick_params(axis='x', rotation=45)
            ax.legend(title=target_col, labels=['Class 0', 'Class 1'])
            
            # Th√™m t·ªïng s·ªë m·∫´u tr√™n m·ªói nh√≥m
            totals = df[feature].fillna('MISSING').value_counts()
            for i, total in enumerate(totals):
                ax.text(i, 102, f'n={total}', ha='center', fontsize=8)
        
        plt.suptitle(f'PH√ÇN T√çCH T∆Ø∆†NG QUAN V·ªöI BI·∫æN M·ª§C TI√äU: {target_col}', 
                    fontsize=14, fontweight='bold', y=1.05)
        plt.tight_layout()
        plt.show()
        
        # In th√¥ng tin chi ti·∫øt v·ªÅ m·ªëi quan h·ªá v·ªõi target
        print(f"\nüìå T·ª∑ l·ªá m·ª•c ti√™u theo t·ª´ng nh√≥m:")
        for feature in top_features[:2]:  # Ch·ªâ ph√¢n t√≠ch 2 features
            print(f"\n   {feature}:")
            crosstab_counts = pd.crosstab(df[feature].fillna('MISSING'), df[target_col])
            crosstab_pct = pd.crosstab(df[feature].fillna('MISSING'), 
                                        df[target_col], normalize='index')
            
            for category in crosstab_counts.index[:5]:  # Hi·ªÉn th·ªã top 5 categories
                count_0 = crosstab_counts.loc[category, 0]
                count_1 = crosstab_counts.loc[category, 1]
                pct_1 = crosstab_pct.loc[category, 1] * 100
                print(f"      ‚Ä¢ {category[:20]:20s}: "
                        f"Class 0: {count_0:5,d} | Class 1: {count_1:5,d} "
                        f"({pct_1:5.1f}% default)")
    
def bao_cao_tong_quan_categorical(df, target_col='TARGET'):
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    print("\n" + "‚ïê" * 80)
    print("üìã B√ÅO C√ÅO T·ªîNG QUAN ƒê·∫∂C TR∆ØNG PH√ÇN LO·∫†I")
    print("‚ïê" * 80)
    
    # T·∫°o dataframe t·ªïng quan
    summary_data = []
    for feature in categorical_cols:
        unique_count = df[feature].nunique()
        missing_count = df[feature].isnull().sum()
        missing_pct = (missing_count / len(df)) * 100
        vc = df[feature].value_counts(dropna=True)
        most_common = vc.index[0] if not vc.empty else "N/A"
        most_common_pct = (df[feature].value_counts().iloc[0] / len(df)) * 100 if unique_count > 0 else 0
        available_cat = [col for col in categorical_cols if col != target_col]
        summary_data.append({
            'Feature': feature,
            'Unique Values': unique_count,
            'Missing': f"{missing_count:,} ({missing_pct:.1f}%)",
            'Most Common': f"{most_common} ({most_common_pct:.1f}%)",
            'In Key Features': '‚úì' if feature in available_cat else ''
        })
    
    summary_df = pd.DataFrame(summary_data)
    print(f"\nT·ªïng s·ªë ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i: {len(summary_df)}")
    print(f"\n{summary_df.to_string(index=False)}")
    
    # Khuy·∫øn ngh·ªã
    print("\n" + "üí° KHUY·∫æN NGH·ªä:")
    print("-" * 40)
    
    high_cardinality = [f for f in categorical_cols if df[f].nunique() > 50]
    if high_cardinality:
        print(f"‚ö†Ô∏è  ƒê·∫∑c tr∆∞ng c√≥ cardinality cao (>50): {', '.join(high_cardinality)}")
        print("   ‚Üí Xem x√©t: Grouping, Target Encoding, ho·∫∑c b·ªè qua")
    
    high_missing = [f for f in categorical_cols if df[f].isnull().mean() > 0.3]
    if high_missing:
        print(f"‚ö†Ô∏è  ƒê·∫∑c tr∆∞ng c√≥ nhi·ªÅu missing (>30%): {', '.join(high_missing)}")
        print("   ‚Üí Xem x√©t: Imputation ho·∫∑c lo·∫°i b·ªè")
    
    low_cardinality = [f for f in categorical_cols if df[f].nunique() == 2]
    if low_cardinality:
        print(f"‚úÖ ƒê·∫∑c tr∆∞ng binary t·ªët cho encoding: {', '.join(low_cardinality)}")
        print("   ‚Üí C√≥ th·ªÉ d√πng Label Encoding")
    
    print("\n" + "‚úÖ PH√ÇN T√çCH HO√ÄN T·∫§T!")

def correlation_analysis(df, target_col='TARGET', path=None):
   
    print("\n" + "="*80)
    print("CORRELATION ANALYSIS")
    print("="*80)
    
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numerical_cols) > 1:
        corr_matrix = df[numerical_cols].corr()
        
        # Show top correlations with target
        if target_col in numerical_cols:
            target_corr = corr_matrix[target_col].sort_values(ascending=False)
            print(f"\nTop 15 Features Correlated with {target_col}:")
            print(target_corr.head(15))
        
        # Plot correlation heatmap for key features
        key_features = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_GOODS_PRICE',
                       'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',
                       'IS_RETIRED_NO_OCCUPATION', 'IS_WORKING_NO_OCCUPATION']
        if target_col in df.columns:
            key_features.append(target_col)
        
        available_features = [f for f in key_features if f in numerical_cols]
        
        if len(available_features) > 1:
            plt.figure(figsize=(12, 10))
            sns.heatmap(df[available_features].corr(), annot=True, fmt='.2f', 
                       cmap='coolwarm', center=0, square=True, linewidths=1)
            plt.title('Correlation Heatmap of Key Features')
            plt.tight_layout()
            plt.savefig(f'{path}/correlation_heatmap.png', dpi=300, bbox_inches='tight')
            plt.close()
            print("\nCorrelation heatmap saved as 'correlation_heatmap.png'")

def engineered_features_analysis(df):
    """Analyze the engineered features"""
    print("\n" + "="*80)
    print("ENGINEERED FEATURES ANALYSIS")
    print("="*80)
    
    engineered_features = {
        'T·ªâ l·ªá vay so v·ªõi nhu c·∫ßu': 'Credit to Goods Price Ratio',
        'S·ªü h·ªØu xe': 'Car Ownership',
        'OCCUPATION_TYPE_ENHANCED': 'Enhanced Occupation Type',
        'IS_RETIRED_NO_OCCUPATION': 'Retired without Occupation Flag',
        'IS_WORKING_NO_OCCUPATION': 'Working without Occupation Flag'
    }
    
    available_eng = {k: v for k, v in engineered_features.items() if k in df.columns}
    
    print(f"\nEngineered Features Analysis:")
    for feature, description in available_eng.items():
        print(f"\n{feature} ({description}):")
        if df[feature].dtype == 'object':
            print(df[feature].value_counts())
        else:
            print(f"  Mean: {df[feature].mean():.4f}")
            print(f"  Median: {df[feature].median():.4f}")
            print(f"  Std: {df[feature].std():.4f}")

def generate_summary_report(df):
    """Generate a summary report"""
    print("\n" + "="*80)
    print("SUMMARY REPORT")
    print("="*80)
    
    print(f"\nDataset Shape: {df.shape}")
    print(f"Total Features: {df.shape[1]}")
    print(f"Total Samples: {df.shape[0]}")
    print(f"\nNumerical Features: {len(df.select_dtypes(include=[np.number]).columns)}")
    print(f"Categorical Features: {len(df.select_dtypes(include=['object']).columns)}")
    print(f"\nTotal Missing Values: {df.isnull().sum().sum()}")
    print(f"Duplicate Rows: {df.duplicated().sum()}")



